MAML: Device is set to :cuda:0
the name is :  ML1
the name of training task is :  <SawyerReachEnvV2 instance>
2024-12-24 22:43:13,940	INFO worker.py:1821 -- Started a local Ray instance.
[36m(pid=244508)[0m
[33m(raylet)[0m [2024-12-24 22:44:13,899 E 244388 244388] (raylet) node_manager.cc:3069: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 510551bf532abfda5df12efaae602d1ccffa4deec840fa09ee6e025e, IP: 10.180.80.122) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 10.180.80.122`
[33m(raylet)[0m
[33m(raylet)[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.
